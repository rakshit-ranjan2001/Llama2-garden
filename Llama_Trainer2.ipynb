{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1c9VKUhLIn1Y1XOhCXv4UJqaFZNqNmXgx","authorship_tag":"ABX9TyPu+s0vESXjb5WDx6rUGUwx"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"e4bf5739c0fa46a790acbd1deac1be2d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_64db9864ee4d415d968d72c9b7fab311","IPY_MODEL_4ad1c4babf4f4c608790cac4e2b3cf13","IPY_MODEL_4f057b652a4b47eb9be9d55635380b42"],"layout":"IPY_MODEL_16ed1ced26ba4b4b99db181cec9749ad"}},"64db9864ee4d415d968d72c9b7fab311":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c319f11907764cb28d59c92068184b0d","placeholder":"​","style":"IPY_MODEL_665ad574209542c0a72b909389b7be16","value":"Loading checkpoint shards: 100%"}},"4ad1c4babf4f4c608790cac4e2b3cf13":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_43ddcffe109742a78334e53dccc20433","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6a9583d2e5b3492582bfc37ee5bea0d0","value":2}},"4f057b652a4b47eb9be9d55635380b42":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9f4ad242525843b1b1f1abc5b935ae28","placeholder":"​","style":"IPY_MODEL_673734bc2b724c23865364c4916cdab2","value":" 2/2 [01:01&lt;00:00, 27.93s/it]"}},"16ed1ced26ba4b4b99db181cec9749ad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c319f11907764cb28d59c92068184b0d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"665ad574209542c0a72b909389b7be16":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"43ddcffe109742a78334e53dccc20433":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6a9583d2e5b3492582bfc37ee5bea0d0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9f4ad242525843b1b1f1abc5b935ae28":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"673734bc2b724c23865364c4916cdab2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","source":["# import pandas as pd\n","# df = pd.read_csv('/content/drive/MyDrive/Projects/Llama2_Garden/gardening_dataset.csv')\n","# df.to_parquet('/content/drive/MyDrive/Projects/Llama2_Garden/gardening_dataset.parquet')"],"metadata":{"id":"UJXt9-vhg0J7","executionInfo":{"status":"ok","timestamp":1705660209552,"user_tz":-330,"elapsed":20,"user":{"displayName":"Rakshit Ranjan","userId":"06314977099579992897"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["cd /content/drive/MyDrive/Projects/Llama2_Garden"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OS9fEq_ufwER","executionInfo":{"status":"ok","timestamp":1705660209553,"user_tz":-330,"elapsed":17,"user":{"displayName":"Rakshit Ranjan","userId":"06314977099579992897"}},"outputId":"6ad1264c-a495-43eb-f55a-ea14c4ae8dcc"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Projects/Llama2_Garden\n"]}]},{"cell_type":"code","source":["!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 trl==0.4.7 transformers==4.36.1"],"metadata":{"id":"ZRYPzY1jZ4Gb","executionInfo":{"status":"ok","timestamp":1705660214942,"user_tz":-330,"elapsed":5403,"user":{"displayName":"Rakshit Ranjan","userId":"06314977099579992897"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["import os\n","import torch\n","from datasets import load_dataset\n","from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    BitsAndBytesConfig,\n","    HfArgumentParser,\n","    TrainingArguments,\n","    pipeline,\n","    logging,\n","    Conversation\n",")\n","from peft import LoraConfig, PeftModel\n","from trl import SFTTrainer"],"metadata":{"id":"INdpgTnCZ8Av","executionInfo":{"status":"ok","timestamp":1705660229215,"user_tz":-330,"elapsed":14290,"user":{"displayName":"Rakshit Ranjan","userId":"06314977099579992897"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# The model that you want to train from the Hugging Face hub\n","model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n","# model_name = \"/content/drive/MyDrive/Projects/Llama2_Garden/llama-2-7b-chat.Q8_0.gguf\"\n","\n","# The instruction dataset to use\n","dataset_name = \"/content/drive/MyDrive/Projects/Llama2_Garden/Dataset\"\n","\n","# Fine-tuned model name\n","new_model = \"Yggdrasil-L27BC-V2\"\n","\n","# ################################################################################\n","# # QLoRA parameters\n","# ################################################################################\n","\n","# # LoRA attention dimension\n","# lora_r = 64\n","\n","# # Alpha parameter for LoRA scaling\n","# lora_alpha = 16\n","\n","# # Dropout probability for LoRA layers\n","# lora_dropout = 0.1\n","\n","# ################################################################################\n","# # bitsandbytes parameters\n","# ################################################################################\n","\n","# # Activate 4-bit precision base model loading\n","# use_4bit = True\n","\n","# # Compute dtype for 4-bit base models\n","# bnb_4bit_compute_dtype = \"float16\"\n","\n","# # Quantization type (fp4 or nf4)\n","# bnb_4bit_quant_type = \"nf4\"\n","\n","# # Activate nested quantization for 4-bit base models (double quantization)\n","# use_nested_quant = False\n","\n","# ################################################################################\n","# # TrainingArguments parameters\n","# ################################################################################\n","\n","# # Output directory where the model predictions and checkpoints will be stored\n","# output_dir = \"./results\"\n","\n","# # Number of training epochs\n","# num_train_epochs = 1\n","\n","# # Enable fp16/bf16 training (set bf16 to True with an A100)\n","# fp16 = False\n","# bf16 = False\n","\n","# # Batch size per GPU for training\n","# per_device_train_batch_size = 4\n","\n","# # Batch size per GPU for evaluation\n","# per_device_eval_batch_size = 4\n","\n","# # Number of update steps to accumulate the gradients for\n","# gradient_accumulation_steps = 1\n","\n","# # Enable gradient checkpointing\n","# gradient_checkpointing = True\n","\n","# # Maximum gradient normal (gradient clipping)\n","# max_grad_norm = 0.3\n","\n","# # Initial learning rate (AdamW optimizer)\n","# learning_rate = 2e-4\n","\n","# # Weight decay to apply to all layers except bias/LayerNorm weights\n","# weight_decay = 0.001\n","\n","# # Optimizer to use\n","# optim = \"paged_adamw_32bit\"\n","\n","# # Learning rate schedule\n","# lr_scheduler_type = \"cosine\"\n","\n","# # Number of training steps (overrides num_train_epochs)\n","# max_steps = -1\n","\n","# # Ratio of steps for a linear warmup (from 0 to learning rate)\n","# warmup_ratio = 0.03\n","\n","# # Group sequences into batches with same length\n","# # Saves memory and speeds up training considerably\n","# group_by_length = True\n","\n","# # Save checkpoint every X updates steps\n","# save_steps = 0\n","\n","# # Log every X updates steps\n","# logging_steps = 25\n","\n","# ################################################################################\n","# # SFT parameters\n","# ################################################################################\n","\n","# # Maximum sequence length to use\n","# max_seq_length = None\n","\n","# # Pack multiple short examples in the same input sequence to increase efficiency\n","# packing = False\n","\n","# Load the entire model on the GPU 0\n","device_map = {\"\": 0}"],"metadata":{"id":"ib_We3NLtj2E","executionInfo":{"status":"ok","timestamp":1705660229216,"user_tz":-330,"elapsed":15,"user":{"displayName":"Rakshit Ranjan","userId":"06314977099579992897"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# # Load dataset (you can process it here)\n","# dataset = load_dataset(dataset_name, split=\"train\")\n","\n","# # Load tokenizer and model with QLoRA configuration\n","# compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n","\n","# bnb_config = BitsAndBytesConfig(\n","#     load_in_4bit=use_4bit,\n","#     bnb_4bit_quant_type=bnb_4bit_quant_type,\n","#     bnb_4bit_compute_dtype=compute_dtype,\n","#     bnb_4bit_use_double_quant=use_nested_quant,\n","# )\n","\n","# # Check GPU compatibility with bfloat16\n","# if compute_dtype == torch.float16 and use_4bit:\n","#     major, _ = torch.cuda.get_device_capability()\n","#     if major >= 8:\n","#         print(\"=\" * 80)\n","#         print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n","#         print(\"=\" * 80)\n","\n","# # Load base model\n","# model = AutoModelForCausalLM.from_pretrained(\n","#     model_name,\n","#     quantization_config=bnb_config,\n","#     device_map=device_map\n","# )\n","# model.config.use_cache = False\n","# model.config.pretraining_tp = 1\n","\n","# # Load LLaMA tokenizer\n","# tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n","# tokenizer.pad_token = tokenizer.eos_token\n","# tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n","\n","# # Load LoRA configuration\n","# peft_config = LoraConfig(\n","#     lora_alpha=lora_alpha,\n","#     lora_dropout=lora_dropout,\n","#     r=lora_r,\n","#     bias=\"none\",\n","#     task_type=\"CAUSAL_LM\",\n","# )\n","\n","# # Set training parameters\n","# training_arguments = TrainingArguments(\n","#     output_dir=output_dir,\n","#     num_train_epochs=num_train_epochs,\n","#     per_device_train_batch_size=per_device_train_batch_size,\n","#     gradient_accumulation_steps=gradient_accumulation_steps,\n","#     optim=optim,\n","#     save_steps=save_steps,\n","#     logging_steps=logging_steps,\n","#     learning_rate=learning_rate,\n","#     weight_decay=weight_decay,\n","#     fp16=fp16,\n","#     bf16=bf16,\n","#     max_grad_norm=max_grad_norm,\n","#     max_steps=max_steps,\n","#     warmup_ratio=warmup_ratio,\n","#     group_by_length=group_by_length,\n","#     lr_scheduler_type=lr_scheduler_type,\n","#     report_to=\"tensorboard\"\n","# )\n","\n","# # Set supervised fine-tuning parameters\n","# trainer = SFTTrainer(\n","#     model=model,\n","#     train_dataset=dataset,\n","#     peft_config=peft_config,\n","#     dataset_text_field=\"text\",\n","#     max_seq_length=max_seq_length,\n","#     tokenizer=tokenizer,\n","#     args=training_arguments,\n","#     packing=packing,\n","# )\n","\n","# # Train model\n","# trainer.train()\n","\n","# # Save trained model\n","# trainer.model.save_pretrained(new_model)"],"metadata":{"id":"dEpsiMCr6xCd","executionInfo":{"status":"ok","timestamp":1705660229216,"user_tz":-330,"elapsed":14,"user":{"displayName":"Rakshit Ranjan","userId":"06314977099579992897"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# del base_model\n","# del pipe\n","torch.cuda.empty_cache()\n","# del trainer\n","# import gc\n","# gc.collect()\n","# gc.collect()"],"metadata":{"id":"zy0IGrunsA5h","executionInfo":{"status":"ok","timestamp":1705660229216,"user_tz":-330,"elapsed":14,"user":{"displayName":"Rakshit Ranjan","userId":"06314977099579992897"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["base_model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    low_cpu_mem_usage=True,\n","    return_dict=True,\n","    torch_dtype=torch.float16,\n","    device_map=device_map,\n",")\n","model = PeftModel.from_pretrained(base_model, new_model)\n","model = model.merge_and_unload()\n","\n","# Reload tokenizer to save it\n","tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n","tokenizer.pad_token = tokenizer.eos_token\n","tokenizer.padding_side = \"right\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":138,"referenced_widgets":["e4bf5739c0fa46a790acbd1deac1be2d","64db9864ee4d415d968d72c9b7fab311","4ad1c4babf4f4c608790cac4e2b3cf13","4f057b652a4b47eb9be9d55635380b42","16ed1ced26ba4b4b99db181cec9749ad","c319f11907764cb28d59c92068184b0d","665ad574209542c0a72b909389b7be16","43ddcffe109742a78334e53dccc20433","6a9583d2e5b3492582bfc37ee5bea0d0","9f4ad242525843b1b1f1abc5b935ae28","673734bc2b724c23865364c4916cdab2"]},"id":"b-fEqFGpstSQ","executionInfo":{"status":"ok","timestamp":1705660326593,"user_tz":-330,"elapsed":97390,"user":{"displayName":"Rakshit Ranjan","userId":"06314977099579992897"}},"outputId":"96527b1f-fb45-4890-cf55-8c2130128cad"},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e4bf5739c0fa46a790acbd1deac1be2d"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:394: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["pipe = pipeline(task=\"conversational\", model=model, tokenizer=tokenizer, max_new_tokens=750)"],"metadata":{"id":"UMWutQWx2ett","executionInfo":{"status":"ok","timestamp":1705660326594,"user_tz":-330,"elapsed":47,"user":{"displayName":"Rakshit Ranjan","userId":"06314977099579992897"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["def get_reply(prompt, conversation):\n","  conversation.add_message(message = {\n","      \"role\": \"user\",\n","      \"content\": prompt\n","  })\n","  conversation = pipe(conversation)\n","  result = conversation.messages[-1][\"content\"]\n","  return result"],"metadata":{"id":"1AYu8aIbASf7","executionInfo":{"status":"ok","timestamp":1705660326595,"user_tz":-330,"elapsed":11,"user":{"displayName":"Rakshit Ranjan","userId":"06314977099579992897"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["conversation = Conversation([{\n","    \"role\": \"system\",\n","    \"content\": \"\"\"You are a gardening assistant bot. Your primary task is to answer questions based STRICTLY on gardening.\n","\n","      RULES:\n","      - ONLY answer if the question relates directly to the gardening.\n","      - Ensure the answers are relevant and helpful in the context of gardening.\n","      - If the question is a greeting (like \"Hello\", \"Hi\", etc.), respond with a corresponding greeting.\"\"\"}])\n","while True:\n","  prompt = input(\"User: \")\n","  if prompt == \"exit\":\n","    print(\"Thank you for gardening with us\")\n","    break\n","  res = get_reply(prompt, conversation)\n","  print(f\"Assistant: \",res,\"\\n\\n\")"],"metadata":{"id":"j-qv3gZYAcAv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1705660505459,"user_tz":-330,"elapsed":178874,"user":{"displayName":"Rakshit Ranjan","userId":"06314977099579992897"}},"outputId":"cfb027c0-e72e-41e6-be3b-44ad17e2975d"},"execution_count":11,"outputs":[{"name":"stdout","output_type":"stream","text":["User: how to climb a tree\n"]},{"output_type":"stream","name":"stderr","text":["\n","No chat template is defined for this tokenizer - using the default template for the LlamaTokenizerFast class. If the default is not appropriate for your model, please set `tokenizer.chat_template` to an appropriate template. See https://huggingface.co/docs/transformers/main/chat_templating for more information.\n","\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1518: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Assistant:   Hello! Climbing a tree can be a fun and rewarding experience, but it's important to do it safely. Here are some steps you can follow to climb a tree:\n","\n","1. Choose a tree that is sturdy and has branches that are easy to grab onto. Look for trees with a wide trunk and branches that are spaced out evenly.\n","2. Make sure you have the right equipment. You'll need a sturdy ladder or a rope and harness. If you're using a ladder, make sure it's sturdy and securely positioned.\n","3. Start by climbing up the ladder or rope. Make sure you're securely attached to the ladder or rope and that you're not going to slip.\n","4. Once you're at the top of the ladder or rope, take a moment to look around and make sure you're safe. Check for any branches that are loose or weak, and make sure you're not going to fall.\n","5. Now you can start climbing the tree. Make sure you're holding onto the ladder or rope at all times, and that you're not going to slip.\n","6. As you climb, look for branches that are sturdy and easy to grab onto. Make sure you're not going to break any branches, and that you're not going to fall.\n","7. Once you're at the top of the tree, take a moment to look around and enjoy the view. Make sure you're securely attached to the ladder or rope, and that you're not going to fall.\n","8. Finally, make sure you know how to get down safely. If you're using a ladder, make sure you know how to climb down safely. If you're using a rope and harness, make sure you know how to rappel safely.\n","\n","Remember, climbing a tree can be dangerous, so make sure you're prepared and take the necessary safety precautions. Always climb with a partner, and make sure you're both securely attached to the ladder or rope. And most importantly, have fun!\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"," \n","\n","\n","User: How to grow beetroots\n","Assistant:   Growing beets is a relatively easy process, and they can be grown in a variety of conditions. Here are the basic steps for growing beets:\n","\n","1. Choose a location: Beets prefer full sun and well-drained soil. They can tolerate a wide range of soil types, but they do best in soil that is slightly acidic (pH 6.0-6.8).\n","2. Prepare the soil: Beets are a cool-season crop, so they do best in cool, moist soil. If your soil is heavy clay or sandy, you may need to amend it with organic matter to improve its structure.\n","3. Sow the seeds: Sow beet seeds 1/4 inch deep and 1 inch apart in rows that are 18-24 inches apart. You can also sow them in a raised bed or container.\n","4. Water the seeds: Water the seeds gently but thoroughly after sowing. Keep the soil consistently moist during the first few weeks after sowing.\n","5. Thin the seedlings: Once the seedlings have emerged, thin them to 2-3 inches apart. This will give the remaining seedlings enough room to grow.\n","6. Water and fertilize: Water the beets regularly, especially during hot weather. Beets are heavy feeders, so they will benefit from regular fertilization.\n","7. Harvest: Beets are ready to harvest when they are between 1 and 2 inches in diameter. Use a garden fork to carefully dig them up, taking care not to damage the roots.\n","\n","Some tips for growing beets:\n","\n","* Beets are a cool-season crop, so they do best in temperatures between 40°F and 70°F.\n","* Beets are a hardy crop, so they can tolerate some frost. However, they do not do well in hot weather.\n","* Beets are a heavy feeder, so they will benefit from regular fertilization.\n","* Beets are a slow-growing crop, so they may take longer to mature than other vegetables.\n","* Beets are a versatile crop, and they can be used in a variety of dishes, including salads, soups, and pickles.\n","\n","Some common problems that can occur when growing beets include:\n","\n","* Root rot: This is caused by a fungus that can infect the roots of the beet plant. It can be prevented by providing good air circulation and by avoiding overwatering.\n","* Leaf spot: This is caused by a fungus that can infect the leaves of the beet plant. It can be prevented by providing good air circulation and by avoiding overwatering.\n","* Root maggots: These are small insects that can infest the roots of the beet plant. They can be prevented by using row covers and by avoiding overwatering.\n","* Aphids: These are small insects that can infest the leaves of the beet plant. They can be prevented by using insecticidal soap and by avoiding overwatering.\n","\n","Some varieties of beets that are commonly grown include:\n","\n","* 'Bloomsdale' - This is a popular variety that is known for its large, sweet roots.\n","* 'Detroit Dark Red' - This is a popular variety that is known for \n","\n","\n","User: exit\n","Thank you for gardening with us\n"]}]},{"cell_type":"code","source":["# questions =[\n","#     \"how to grow potatoes\",\n","#     \"how to grow basil\",\n","#     \"How to play cricket\",\n","#     \"How to climb a tree\"\n","# ]\n","# for q in questions:\n","#   print(\"User: \", q)\n","#   res = get_reply(q, conversation)\n","#   print(\"Assistant: \", res, \"\\n\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":425},"id":"s72TlvgvWWXR","executionInfo":{"status":"error","timestamp":1705660513379,"user_tz":-330,"elapsed":7956,"user":{"displayName":"Rakshit Ranjan","userId":"06314977099579992897"}},"outputId":"c225f478-0896-4794-d39c-9e7b704e0cf1"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["User:  how to grow potatoes\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-6b914778211f>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mquestions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"User: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m   \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_reply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconversation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Assistant: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-10-205e68117106>\u001b[0m in \u001b[0;36mget_reply\u001b[0;34m(prompt, conversation)\u001b[0m\n\u001b[1;32m      4\u001b[0m       \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   })\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0mconversation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconversation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"content\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/conversational.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, conversations, num_workers, **kwargs)\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconversations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconversations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m             \u001b[0mconversations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mConversation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mconv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconversations\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconversations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_workers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1138\u001b[0m             )\n\u001b[1;32m   1139\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1140\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_multi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mrun_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m         \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpreprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1147\u001b[0;31m         \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1148\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1044\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0minference_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m                     \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1046\u001b[0;31m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1047\u001b[0m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/conversational.py\u001b[0m in \u001b[0;36m_forward\u001b[0;34m(self, model_inputs, minimum_tokens, **generate_kwargs)\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"max_length\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgenerate_kwargs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"max_new_tokens\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m             \u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"max_new_tokens\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m         \u001b[0moutput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_encoder_decoder\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m             \u001b[0mstart_position\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1716\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgeneration_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mGenerationMode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGREEDY_SEARCH\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1717\u001b[0m             \u001b[0;31m# 11. run greedy search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1718\u001b[0;31m             return self.greedy_search(\n\u001b[0m\u001b[1;32m   1719\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1720\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgreedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2634\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2635\u001b[0m                 \u001b[0;31m# stop when each sentence is finished\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2636\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0munfinished_sequences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2637\u001b[0m                     \u001b[0mthis_peer_finished\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["conversation"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I0ijxnDKWZmK","executionInfo":{"status":"ok","timestamp":1705660519479,"user_tz":-330,"elapsed":842,"user":{"displayName":"Rakshit Ranjan","userId":"06314977099579992897"}},"outputId":"1a359290-d3b1-46dc-d99c-73d54237ff3b"},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Conversation id: db8518ed-6840-43ae-a061-7ff348a05b46\n","system: You are a gardening assistant bot. Your primary task is to answer questions based STRICTLY on gardening.\n","\n","      RULES:\n","      - ONLY answer if the question relates directly to the gardening.\n","      - Ensure the answers are relevant and helpful in the context of gardening.\n","      - If the question is a greeting (like \"Hello\", \"Hi\", etc.), respond with a corresponding greeting.\n","user: how to climb a tree\n","assistant:  Hello! Climbing a tree can be a fun and rewarding experience, but it's important to do it safely. Here are some steps you can follow to climb a tree:\n","\n","1. Choose a tree that is sturdy and has branches that are easy to grab onto. Look for trees with a wide trunk and branches that are spaced out evenly.\n","2. Make sure you have the right equipment. You'll need a sturdy ladder or a rope and harness. If you're using a ladder, make sure it's sturdy and securely positioned.\n","3. Start by climbing up the ladder or rope. Make sure you're securely attached to the ladder or rope and that you're not going to slip.\n","4. Once you're at the top of the ladder or rope, take a moment to look around and make sure you're safe. Check for any branches that are loose or weak, and make sure you're not going to fall.\n","5. Now you can start climbing the tree. Make sure you're holding onto the ladder or rope at all times, and that you're not going to slip.\n","6. As you climb, look for branches that are sturdy and easy to grab onto. Make sure you're not going to break any branches, and that you're not going to fall.\n","7. Once you're at the top of the tree, take a moment to look around and enjoy the view. Make sure you're securely attached to the ladder or rope, and that you're not going to fall.\n","8. Finally, make sure you know how to get down safely. If you're using a ladder, make sure you know how to climb down safely. If you're using a rope and harness, make sure you know how to rappel safely.\n","\n","Remember, climbing a tree can be dangerous, so make sure you're prepared and take the necessary safety precautions. Always climb with a partner, and make sure you're both securely attached to the ladder or rope. And most importantly, have fun!\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","user: How to grow beetroots\n","assistant:  Growing beets is a relatively easy process, and they can be grown in a variety of conditions. Here are the basic steps for growing beets:\n","\n","1. Choose a location: Beets prefer full sun and well-drained soil. They can tolerate a wide range of soil types, but they do best in soil that is slightly acidic (pH 6.0-6.8).\n","2. Prepare the soil: Beets are a cool-season crop, so they do best in cool, moist soil. If your soil is heavy clay or sandy, you may need to amend it with organic matter to improve its structure.\n","3. Sow the seeds: Sow beet seeds 1/4 inch deep and 1 inch apart in rows that are 18-24 inches apart. You can also sow them in a raised bed or container.\n","4. Water the seeds: Water the seeds gently but thoroughly after sowing. Keep the soil consistently moist during the first few weeks after sowing.\n","5. Thin the seedlings: Once the seedlings have emerged, thin them to 2-3 inches apart. This will give the remaining seedlings enough room to grow.\n","6. Water and fertilize: Water the beets regularly, especially during hot weather. Beets are heavy feeders, so they will benefit from regular fertilization.\n","7. Harvest: Beets are ready to harvest when they are between 1 and 2 inches in diameter. Use a garden fork to carefully dig them up, taking care not to damage the roots.\n","\n","Some tips for growing beets:\n","\n","* Beets are a cool-season crop, so they do best in temperatures between 40°F and 70°F.\n","* Beets are a hardy crop, so they can tolerate some frost. However, they do not do well in hot weather.\n","* Beets are a heavy feeder, so they will benefit from regular fertilization.\n","* Beets are a slow-growing crop, so they may take longer to mature than other vegetables.\n","* Beets are a versatile crop, and they can be used in a variety of dishes, including salads, soups, and pickles.\n","\n","Some common problems that can occur when growing beets include:\n","\n","* Root rot: This is caused by a fungus that can infect the roots of the beet plant. It can be prevented by providing good air circulation and by avoiding overwatering.\n","* Leaf spot: This is caused by a fungus that can infect the leaves of the beet plant. It can be prevented by providing good air circulation and by avoiding overwatering.\n","* Root maggots: These are small insects that can infest the roots of the beet plant. They can be prevented by using row covers and by avoiding overwatering.\n","* Aphids: These are small insects that can infest the leaves of the beet plant. They can be prevented by using insecticidal soap and by avoiding overwatering.\n","\n","Some varieties of beets that are commonly grown include:\n","\n","* 'Bloomsdale' - This is a popular variety that is known for its large, sweet roots.\n","* 'Detroit Dark Red' - This is a popular variety that is known for\n","user: how to grow potatoes"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"id":"fs1PpOjShqUN","executionInfo":{"status":"aborted","timestamp":1705660513382,"user_tz":-330,"elapsed":13,"user":{"displayName":"Rakshit Ranjan","userId":"06314977099579992897"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"_kAhXbwTh4Ej","executionInfo":{"status":"aborted","timestamp":1705660513383,"user_tz":-330,"elapsed":14,"user":{"displayName":"Rakshit Ranjan","userId":"06314977099579992897"}}},"execution_count":null,"outputs":[]}]}